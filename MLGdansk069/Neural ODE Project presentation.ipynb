{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.enable_eager_execution ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Neural Ordinary Differential Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Two different ways to model things\n",
    "\n",
    "When considering a problem of process modeling we - usually - have set of N data points pairs ${(x_n, y_n)}; 1 \\le n \\le N$. This dataset comes from observing some process out in the wild. What we would like to do is to build mapping $f$ that will approximate the very process we observed. \n",
    "\n",
    "We will compare two approaches - quite popular **regression** and the topic of this presentation **ordinary differential equations**.\n",
    "\n",
    "### Regression\n",
    "\n",
    "We can write regression in general from as:\n",
    "$$Y = f(X, \\beta) + \\epsilon$$\n",
    "\n",
    "Of course, $f$ can take various form - from simple matrix multiplication to extremally complex like complicated neural nets. What is most important is the fact that we describe the relationship _directly_. \n",
    "\n",
    "\n",
    "### Ordinary Differential Equations\n",
    "\n",
    "On the other hand, when using the ODE approach instead of using a _direct_ approach, we can observe _rate of change_ - which in mathematical terms means we are toying with derivatives. We can describe this relationship as:\n",
    "$$\\frac{dy}{dx} = f'(x),$$\n",
    "where f is a function we are trying to approximate. To calculate $f$, we need to integrate it.\n",
    "\n",
    "The general idea is that instead of approximating the function itself, we can approximate it's derivative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## ODE Refresher\n",
    "\n",
    "### What are ODEs?\n",
    "Straight from Wikipedia:\n",
    "> In mathematics, an ordinary differential equation (ODE) is a differential equation containing one or more functions of one independent variable and the derivatives of those functions. The term ordinary is used in contrast with the term partial differential equation which may be with respect to more than one independent variable.\n",
    "\n",
    "Let $y = f(x)$ then equation of the form:\n",
    "$$F(x, y, y', ..., y^{(n-1)})=y^{(n)}$$\n",
    "is called explicit ordinary differential equation of order n.\n",
    "\n",
    "### Initial value problem in ODEs\n",
    "\n",
    "One of the most common problems is that of _initial value problem_. When modeling system this usually means that we want to know how a system will evolve in \"time\" given some initial conditions.\n",
    "\n",
    "#### Sample Problem\n",
    "\n",
    "$$\\frac{dy(t)}{dt} = 10 - t, y(0) = - 1$$\n",
    "\n",
    "###### General Solution\n",
    "\n",
    "$$\\frac{dy(t)}{dt} = 10 - t$$\n",
    "\n",
    "$$dy(t) = (10 - t)dt$$\n",
    "\n",
    "$$\\int dy(t)dt = \\int (10 -t)dt$$\n",
    "\n",
    "$$y = 10t - \\frac{t^2}{2} + C$$\n",
    "\n",
    "###### Specific solution\n",
    "\n",
    "$$-1 = 10(0) - \\frac{0}{2} + C$$\n",
    "\n",
    "$$C = -1$$\n",
    "\n",
    "Solution: $y = 10t - \\frac{t^2}{2} - 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## ResNet and Euler's Method\n",
    "\n",
    "### What is ResNet?\n",
    "\n",
    "ResNet is a network architecture introduced in 2015, taking _deep_ in deep learning to a whole new level. The motivation behind creating this network is highly empirical - creators (not only them) observed issue the named _degradation problem_. The observed that adding layers not only does not improve the overall accuracy of the network but worsens it. That implies that added layers are not able to learn even identity function - which would be enough to keep accuracy on the same level.\n",
    "\n",
    "Let's set that ordered set of layers should should learn some function $g$. If our net is able to learn how to approximate this function then it should be able to learn how to approximate residual function $f(x) = g(x) - x$. From that we have $g(x) = f(x) + x$\n",
    "\n",
    "### Residual Block\n",
    "\n",
    "Residual block can be considered as a mini neural net, where _input of this net is also added to the output of the net_. We may think about it as:\n",
    "$$fr^l(x) = f(x) + Wx$$\n",
    "Where $l$ represents a number of layers in a mini neural net. $W$, on the other hand, is a matrix that allows simple linear transformation, for example, to make \"dimensions compile\". Usually, it's just an identity matrix.\n",
    "\n",
    "#### ResNet as Euler Discretization\n",
    "\n",
    "Now... let's get back to ODEs. One of the simplest approaches to numerically solve ODE is _Euler's Discretization_. Idea behind this method goes as follows:\n",
    "1. Consider $y' = f(x, y)$ given initial condition $y_0 = y(x_0)$\n",
    "2. Let's suppose our step size is of size $t$. We will use this value to update value of $x$: $x_{n+1} = x_{n} + t$.\n",
    "3. From definition of derivative we have $y' = \\frac{\\Delta y}{t}$, simple manipulation gives us $\\Delta y = tf(x_n, y_n)$\n",
    "4. From everything above we get $y_{n+1} = y_n + \\Delta y$ and $y_{n + 1} = y_n + tf(x_n, y_n)$.\n",
    "\n",
    "It turns out we can eaisly represent ResNet block in similar form, particularly like this:\n",
    "$$h(t+1) = h(t) + f(h(t))$$\n",
    "which is extremally similar to:\n",
    "$$y_{n+1} = y_n + \\Delta y$$\n",
    "\n",
    "Hmm... \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### Making Discretization continous\n",
    "\n",
    "Authors of Neural ODE asked a question (not first though) and provided implementable answers to the following question: _what will happen if we make as small steps as possible?_ Making our discretization - continuous. They described the state of the neural network, or rather change of the state in the following form:\n",
    "\n",
    "$$\\frac{dh(t)}{dt} = f(t, h(t); \\theta)$$\n",
    "\n",
    "Where t is _depth_ of our network, which could be imagined as a continuous number of layer. So $h(t_0)$ would be input to the network and $h(t_1)$ could be the output of the network. \n",
    "\n",
    "##### Forward pass \n",
    "\n",
    "For starter let's think how to do forward pass in a network defined as above. The most straight forward approach seems to make sense:\n",
    "\n",
    "$$h(t_1) = h(t_0) + \\int_{t_0}^{t_1}f(t, h(t); \\theta)$$\n",
    "\n",
    "That looks very nice, we can easily write formula for loss function:\n",
    "\n",
    "$$ L(h(t_1), y_{expected}) = L(h(t_0) + \\int_{t_0}^{t_1}f(t, h(t); \\theta), y_{expected}) = L(ODESolve(h(t_0), f, t_0, t_1, \\theta)) $$\n",
    "\n",
    "$ODESolve$ is a _black box ode solver _ from our perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "To fully formulate our network we need a way to somehow update the network's parameters so that our network can learn. \n",
    "\n",
    "### Naive Approach\n",
    "\n",
    "The most naive approach would be just to backpropagate due to: number of steps required to solve an equation, needing to store all the activations, numerical errors, lack of guarantees on differentiability. \n",
    "\n",
    "### What is the adjoint method?\n",
    "\n",
    "Take from: https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/, as this is a simple and neat explanation.\n",
    "\n",
    "Suppose we have 2 known matrices $A$ and $C$, and a known vector u. We would like to compute the product:\n",
    "\n",
    "$$u^TB \\text{ such that } AB = C$$\n",
    "\n",
    "A most natural approach would be to solve a linear system to find an unknown matrix $B$, then compute the product. Let's use mathigical trick to solve this problem. Let's find a vector $v$ and compute:\n",
    "\n",
    "$$ v^TC \\text{ such that } A^Tv = u $$\n",
    "\n",
    "We can reinterpet our initial problem as:\n",
    "\n",
    "$$v^TC = v^TAB = (A^Tv)^TB = u^TB$$\n",
    "\n",
    "Using this transformation, we have reduced the problem from solving for a matrix to solving for a vector.\n",
    "\n",
    "### And what all of this has to do with Neural ODEs?\n",
    "\n",
    "For technical details please see the original Neural ODE paper, especially Appendix B, here we will try to explain the intuition behind it. Based on: https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html.\n",
    "\n",
    "To optimize loss, we need gradients w.r.t. all free parameters (usually represented by $\\theta$), because those are parameters that will be updated.\n",
    "\n",
    "The first step (similarly to \"standard\" backprop) is to compute the gradient of the loss with respect to the hidden states:\n",
    "\n",
    "$$\\dfrac{\\delta L}{\\delta h(t)}$$\n",
    "\n",
    "The hidden state itself is dependent on time/depth, so we also can take derivative w.r.t. to time. Consider that this time derivative will be a _reverse traversal of the hidden states_. This may be numerically intensive - and this is when the adjoint method comes into play.\n",
    "\n",
    "In order to do so we will define _adjoint state vector_, that will keep track of the time dynamics:\n",
    "\n",
    "$$a(t) = -\\dfrac{\\delta L}{\\delta h(t)}$$\n",
    "\n",
    "The authors prove that the adjoint state defined like this is can be passed to _black box ode solver_, to calculate gradients w.r.t $t_0$.\n",
    "\n",
    "$$a(t_0) = \\int_{t_1}^{t_0} -a(t)^T \\dfrac{\\delta f(t, h(t); \\theta_t)}{\\delta h(t)}dt$$.\n",
    "\n",
    "Based on this we are able to define all required gradients, that is:\n",
    "\n",
    "$$\\frac{d L}{d \\theta} =-\\int_{t_1}^{t_0} a(t)^T \\dfrac{\\delta f(t, h(t); \\theta_t)}{\\delta \\theta}dt $$\n",
    "\n",
    "$$\\frac{dL}{dt_1} = a(t_1)f(a(t_1), t_1, \\theta)$$\n",
    "\n",
    "$$\\frac{dL}{dt_0} = a(t_1) -\\int_{t_1}^{t_0} a(t)^T \\dfrac{\\delta f(t, h(t); \\theta_t)}{\\delta t}dt $$\n",
    "\n",
    "To sum it beautifully:\n",
    "\n",
    "> _Backpropagation using the adjoint method: the adjoint state is a vector with its own time-dependent dynamics, only the trajectory runs backward in time. We can solve for the gradients using an ODE solver for the adjoint time derivative, starting at $t_1$._ [source](https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html)\n",
    "\n",
    "### Combining it all together!\n",
    "\n",
    "[source](https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html)\n",
    "\n",
    "How would we solve a problem using this approach?\n",
    "\n",
    "1. Instead of modelling this relationship direcatly, we model derivative:\n",
    "\n",
    "$$\\frac{dy}{dx} = f(x, y)$$\n",
    "\n",
    "2. We parametrise approximation by a neural network with hidden states $h(t)$, with $h(t_0) = x$, $h(t_1) = y$. Function approximation problem now has form of (f is a neural network):\n",
    "\n",
    "$$\\frac{dh(t)}{dt} = f(t, h(t), \\theta)$$\n",
    "\n",
    "\n",
    "3. Based on the above we can numerically integrate state, so we get network response:\n",
    "\n",
    "$$\\hat{y} = h(t_1) = \\int_{t_0}^{t_1} f(t, h(t), \\theta)dt$$.\n",
    "\n",
    "4. Free parameters of our problem are $t_0, t_1, \\theta$. We optimize free parameters with the help of the adjoint method.\n",
    "\n",
    "### Wait - but why?\n",
    "\n",
    "- Adaptive computation\n",
    "- Parameter efficency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "class NeuralODE:\n",
    "    def __init__(self, model: tf.keras.Model, t=[0,1]):\n",
    "        self._t = t\n",
    "        self._model = model\n",
    "        self.__output_dim_product = None\n",
    "        self.__output_gradients_dim_product = None\n",
    "        self.__batch_size = None\n",
    "        self.__number_of_parameters = None\n",
    "        \n",
    "    def forward_pass(self, inputs, rtol=1e-06, atol=1e-12):\n",
    "        pass\n",
    "    \n",
    "    def backward_pass(self, outputs, output_gradients, rtol=1e-06, atol=1e-12):\n",
    "        # Ugly code, should be improved\n",
    "        self.__output_dim_product = self.__number_of_params(outputs)\n",
    "        self.__output_gradients_dim_product = self.__number_of_params(output_gradients)\n",
    "        self.__batch_size = self.__raw_shape(outputs)[0]\n",
    "        flat_state_tensor = self.__flat_state_tensor(output_gradients,outputs)\n",
    "        self.__number_of_parameters = self.__number_of_params(flat_state_tensor) - self.__output_gradients_dim_product - self.__output_dim_product\n",
    "        # TO IMPLEMENT PART\n",
    "        out = None\n",
    "\n",
    "        layers_dims = [self.__raw_shape(weight) for weight in self._model.weights]\n",
    "        layers_size = [self.__number_of_params(weight) for weight in self._model.weights]\n",
    "        adjoint, hidden, gradients = tf.split(\n",
    "            out[-1], \n",
    "            [self.__output_gradients_dim_product , self.__output_dim_product, np.sum(layers_size)]\n",
    "        )\n",
    "        layer_params_dims = zip(tf.split(gradients, layers_size), layers_dims)\n",
    "        reshaped = [tf.reshape(param_dim[0], param_dim[1]) for param_dim in layer_params_dims]\n",
    "        return [\n",
    "            tf.reshape(hidden, [self.__batch_size, -1]), *reshaped\n",
    "        ]\n",
    "\n",
    "    def _backward_dynamics(self, t, flat_state_tensor):\n",
    "        adjoint, hidden, layers = tf.split(\n",
    "            flat_state_tensor, \n",
    "            [self.__output_gradients_dim_product, self.__output_dim_product, self.__number_of_parameters]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        at = -tf.reshape(adjoint, [self.__batch_size, -1])\n",
    "        ht = tf.reshape(hidden, [self.__batch_size, -1])\n",
    "        pass\n",
    "    \n",
    "    def __flat_state_tensor(self, output_gradients, outputs):\n",
    "        grad_weights = [tf.zeros_like(w) for w in self._model.weights]\n",
    "        flat_layers = [tf.reshape(weight, [-1]) for weight in grad_weights]\n",
    "        state = (tf.reshape(output_gradients, [-1]), tf.reshape(outputs, [-1]), *flat_layers)\n",
    "        flat_state_tensor = tf.concat(state, 0)\n",
    "        return flat_state_tensor\n",
    "    \n",
    "        \n",
    "    def __number_of_params(self, tensor):\n",
    "        raw_dims = self.__raw_shape(tensor)\n",
    "        return np.prod(raw_dims)\n",
    "    \n",
    "    def __raw_shape(self, tensor):\n",
    "        s = tensor.get_shape()\n",
    "        raw_dims = tuple([s[i].value for i in range(0, len(s))])\n",
    "        return raw_dims\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Sample problem - MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0818 14:16:18.765696 10392 deprecation.py:323] From <ipython-input-4-8923f4ea19e5>:15: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0818 14:16:18.768652 10392 deprecation.py:323] From C:\\Users\\piotr\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0818 14:16:18.769650 10392 deprecation.py:323] From C:\\Users\\piotr\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 14:16:18.997629 10392 deprecation.py:323] From C:\\Users\\piotr\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0818 14:16:19.000555 10392 deprecation.py:323] From C:\\Users\\piotr\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W0818 14:16:19.054585 10392 deprecation.py:323] From C:\\Users\\piotr\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8923f4ea19e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0min_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mnode_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneural_ode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mlast_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.0001\n",
    "num_steps = 100\n",
    "batch_size = 128\n",
    "display_step = 300\n",
    "\n",
    "# Network Parameters\n",
    "node_dim = 256 # 1st layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "in_layer = tf.keras.layers.Dense(node_dim)\n",
    "out_layer = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "class Lambda(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Lambda, self).__init__(name=\"Module\")\n",
    "        self.dense_1 = tf.keras.layers.Dense(node_dim)\n",
    "        self.dense_2 = tf.keras.layers.Dense(node_dim)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        t, x = inputs\n",
    "        h = self.dense_1(x)\n",
    "        return  self.dense_2(h)\n",
    "\n",
    "neural_ode = NeuralODE(Lambda())\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "loss_history = []\n",
    "for step in range(num_steps):\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    in_result = in_layer(batch_x)\n",
    "    node_output, info_dict = neural_ode.forward_pass(in_result)\n",
    "    last_step = node_output[-1]\n",
    "    prediction = out_layer(last_step)\n",
    "    with tf.GradientTape() as g: \n",
    "        g.watch(prediction)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=batch_y))\n",
    "    loss_history.append(loss.numpy())\n",
    "    dLossdLast = g.gradient(loss, prediction)\n",
    "    dWeightsdOut = tf.matmul(tf.transpose(last_step), dLossdLast)\n",
    "    \n",
    "    dLoss = tf.matmul(dLossdLast, tf.transpose(out_layer.weights[0]))\n",
    "    hidden, *dWeightsdODE = neural_ode.backward_pass(last_step, dLoss) \n",
    "    \n",
    "    back_hidden = tf.matmul(hidden, tf.transpose(neural_ode._model.dense_2.weights[0]))\n",
    "    back_hidden = tf.matmul(back_hidden, neural_ode._model.dense_1.weights[0])\n",
    "    step_back = tf.matmul(back_hidden, tf.transpose(in_layer.weights[0]))\n",
    "    dWeightsdIn = tf.matmul(tf.transpose(step_back), in_result)\n",
    "\n",
    "    optimizer.apply_gradients(zip(dWeightsdODE, neural_ode._model.trainable_weights))\n",
    "    optimizer.apply_gradients(zip([dWeightsdOut, tf.reduce_mean(dLossdLast, axis=0)], out_layer.trainable_weights))\n",
    "    optimizer.apply_gradients(zip([dWeightsdIn, tf.reduce_mean(hidden, axis=0)], in_layer.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPH ERRORS\n",
    "print(loss_history)\n",
    "plt.plot(loss_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### More advanced problems\n",
    "\n",
    "- Scalable and intertivle normalizing flows\n",
    "- Continous time-sries models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Sources:\n",
    " - https://github.com/kmkolasinski/deep-learning-notes/tree/master/seminars/2019-03-Neural-Ordinary-Differential-Equations\n",
    " - https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/\n",
    " - https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html\n",
    " - https://arxiv.org/abs/1806.07366\n",
    " - https://pl.wikipedia.org/wiki/Regresja_(statystyka)\n",
    " - https://en.wikipedia.org/wiki/Ordinary_differential_equation\n",
    " - https://en.wikipedia.org/wiki/Initial_value_problem\n",
    " - http://tutorial.math.lamar.edu/Classes/DE/Modeling.aspx\n",
    " - https://en.wikipedia.org/wiki/Sensitivity_analysis\n",
    " - https://wizardforcel.gitbooks.io/tensorflow-101-sjchoi86/mlp_mnist_simple.html\n",
    " - Reprezentacja stanu sieci neuronowych przy pomocy zwyczajnych równań różniczkowych - Piotr Lewandowski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
