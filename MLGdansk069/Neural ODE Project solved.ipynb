{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.enable_eager_execution ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Neural Ordinary Differential Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Two different ways to model things\n",
    "\n",
    "When considering a problem of process modeling we - usually - have set of N data points pairs ${(x_n, y_n)}; 1 \\le n \\le N$. This dataset comes from observing some process out in the wild. What we would like to do is to build mapping $f$ that will approximate the very process we observed. \n",
    "\n",
    "We will compare two approaches - quite popular **regression** and the topic of this presentation **ordinary differential equations**.\n",
    "\n",
    "### Regression\n",
    "\n",
    "We can write regression in general from as:\n",
    "$$Y = f(X, \\beta) + \\epsilon$$\n",
    "\n",
    "Of course, $f$ can take various form - from simple matrix multiplication to extremally complex like complicated neural nets. What is most important is the fact that we describe the relationship _directly_. \n",
    "\n",
    "\n",
    "### Ordinary Differential Equations\n",
    "\n",
    "On the other hand, when using the ODE approach instead of using a _direct_ approach, we can observe _rate of change_ - which in mathematical terms means we are toying with derivatives. We can describe this relationship as:\n",
    "$$\\frac{dy}{dx} = f'(x),$$\n",
    "where f is a function we are trying to approximate. To calculate $f$, we need to integrate it.\n",
    "\n",
    "The general idea is that instead of approximating the function itself, we can approximate it's derivative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## ODE Refresher\n",
    "\n",
    "### What are ODEs?\n",
    "Straight from Wikipedia:\n",
    "> In mathematics, an ordinary differential equation (ODE) is a differential equation containing one or more functions of one independent variable and the derivatives of those functions. The term ordinary is used in contrast with the term partial differential equation which may be with respect to more than one independent variable.\n",
    "\n",
    "Let $y = f(x)$ then equation of the form:\n",
    "$$F(x, y, y', ..., y^{(n-1)})=y^{(n)}$$\n",
    "is called explicit ordinary differential equation of order n.\n",
    "\n",
    "### Initial value problem in ODEs\n",
    "\n",
    "One of the most common problems is that of _initial value problem_. When modeling system this usually means that we want to know how a system will evolve in \"time\" given some initial conditions.\n",
    "\n",
    "#### Sample Problem\n",
    "\n",
    "$$\\frac{dy(t)}{dt} = 10 - t, y(0) = - 1$$\n",
    "\n",
    "###### General Solution\n",
    "\n",
    "$$\\frac{dy(t)}{dt} = 10 - t$$\n",
    "\n",
    "$$dy(t) = (10 - t)dt$$\n",
    "\n",
    "$$\\int dy(t)dt = \\int (10 -t)dt$$\n",
    "\n",
    "$$y = 10t - \\frac{t^2}{2} + C$$\n",
    "\n",
    "###### Specific solution\n",
    "\n",
    "$$-1 = 10(0) - \\frac{0}{2} + C$$\n",
    "\n",
    "$$C = -1$$\n",
    "\n",
    "Solution: $y = 10t - \\frac{t^2}{2} - 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## ResNet and Euler's Method\n",
    "\n",
    "### What is ResNet?\n",
    "\n",
    "ResNet is a network architecture introduced in 2015, taking _deep_ in deep learning to a whole new level. The motivation behind creating this network is highly empirical - creators (not only them) observed issue the named _degradation problem_. The observed that adding layers not only does not improve the overall accuracy of the network but worsens it. That implies that added layers are not able to learn even identity function - which would be enough to keep accuracy on the same level.\n",
    "\n",
    "Let's set that ordered set of layers should should learn some function $g$. If our net is able to learn how to approximate this function then it should be able to learn how to approximate residual function $f(x) = g(x) - x$. From that we have $g(x) = f(x) + x$\n",
    "\n",
    "### Residual Block\n",
    "\n",
    "Residual block can be considered as a mini neural net, where _input of this net is also added to the output of the net_. We may think about it as:\n",
    "$$fr^l(x) = f(x) + Wx$$\n",
    "Where $l$ represents a number of layers in a mini neural net. $W$, on the other hand, is a matrix that allows simple linear transformation, for example, to make \"dimensions compile\". Usually, it's just an identity matrix.\n",
    "\n",
    "#### ResNet as Euler Discretization\n",
    "\n",
    "Now... let's get back to ODEs. One of the simplest approaches to numerically solve ODE is _Euler's Discretization_. Idea behind this method goes as follows:\n",
    "1. Consider $y' = f(x, y)$ given initial condition $y_0 = y(x_0)$\n",
    "2. Let's suppose our step size is of size $t$. We will use this value to update value of $x$: $x_{n+1} = x_{n} + t$.\n",
    "3. From definition of derivative we have $y' = \\frac{\\Delta y}{t}$, simple manipulation gives us $\\Delta y = tf(x_n, y_n)$\n",
    "4. From everything above we get $y_{n+1} = y_n + \\Delta y$ and $y_{n + 1} = y_n + tf(x_n, y_n)$.\n",
    "\n",
    "It turns out we can eaisly represent ResNet block in similar form, particularly like this:\n",
    "$$h(t+1) = h(t) + f(h(t))$$\n",
    "which is extremally similar to:\n",
    "$$y_{n+1} = y_n + \\Delta y$$\n",
    "\n",
    "Hmm... \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### Making Discretization continous\n",
    "\n",
    "Authors of Neural ODE asked a question (not first though) and provided implementable answers to the following question: _what will happen if we make as small steps as possible?_ Making our discretization - continuous. They described the state of the neural network, or rather change of the state in the following form:\n",
    "\n",
    "$$\\frac{dh(t)}{dt} = f(t, h(t); \\theta)$$\n",
    "\n",
    "Where t is _depth_ of our network, which could be imagined as a continuous number of layer. So $h(t_0)$ would be input to the network and $h(t_1)$ could be the output of the network. \n",
    "\n",
    "##### Forward pass \n",
    "\n",
    "For starter let's think how to do forward pass in a network defined as above. The most straight forward approach seems to make sense:\n",
    "\n",
    "$$h(t_1) = h(t_0) + \\int_{t_0}^{t_1}f(t, h(t); \\theta)$$\n",
    "\n",
    "That looks very nice, we can easily write formula for loss function:\n",
    "\n",
    "$$ L(h(t_1), y_{expected}) = L(h(t_0) + \\int_{t_0}^{t_1}f(t, h(t); \\theta), y_{expected}) = L(ODESolve(h(t_0), f, t_0, t_1, \\theta)) $$\n",
    "\n",
    "$ODESolve$ is a _black box ode solver _ from our perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "To fully formulate our network we need a way to somehow update the network's parameters so that our network can learn. \n",
    "\n",
    "### Naive Approach\n",
    "\n",
    "The most naive approach would be just to backpropagate due to: number of steps required to solve an equation, needing to store all the activations, numerical errors, lack of guarantees on differentiability. \n",
    "\n",
    "### What is the adjoint method?\n",
    "\n",
    "Take from: https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/, as this is a simple and neat explanation.\n",
    "\n",
    "Suppose we have 2 known matrices $A$ and $C$, and a known vector u. We would like to compute the product:\n",
    "\n",
    "$$u^TB \\text{ such that } AB = C$$\n",
    "\n",
    "A most natural approach would be to solve a linear system to find an unknown matrix $B$, then compute the product. Let's use mathigical trick to solve this problem. Let's find a vector $v$ and compute:\n",
    "\n",
    "$$ v^TC \\text{ such that } A^Tv = u $$\n",
    "\n",
    "We can reinterpet our initial problem as:\n",
    "\n",
    "$$v^TC = v^TAB = (A^Tv)^TB = u^TB$$\n",
    "\n",
    "Using this transformation, we have reduced the problem from solving for a matrix to solving for a vector.\n",
    "\n",
    "### And what all of this has to do with Neural ODEs?\n",
    "\n",
    "For technical details please see the original Neural ODE paper, especially Appendix B, here we will try to explain the intuition behind it. Based on: https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html.\n",
    "\n",
    "To optimize loss, we need gradients w.r.t. all free parameters (usually represented by $\\theta$), because those are parameters that will be updated.\n",
    "\n",
    "The first step (similarly to \"standard\" backprop) is to compute the gradient of the loss with respect to the hidden states:\n",
    "\n",
    "$$\\dfrac{\\delta L}{\\delta h(t)}$$\n",
    "\n",
    "The hidden state itself is dependent on time/depth, so we also can take derivative w.r.t. to time. Consider that this time derivative will be a _reverse traversal of the hidden states_. This may be numerically intensive - and this is when the adjoint method comes into play.\n",
    "\n",
    "In order to do so we will define _adjoint state vector_, that will keep track of the time dynamics:\n",
    "\n",
    "$$a(t) = -\\dfrac{\\delta L}{\\delta h(t)}$$\n",
    "\n",
    "The authors prove that the adjoint state defined like this is can be passed to _black box ode solver_, to calculate gradients w.r.t $t_0$.\n",
    "\n",
    "$$a(t_0) = \\int_{t_1}^{t_0} -a(t)^T \\dfrac{\\delta f(t, h(t); \\theta_t)}{\\delta h(t)}dt$$.\n",
    "\n",
    "Based on this we are able to define all required gradients, that is:\n",
    "\n",
    "$$\\frac{d L}{d \\theta} =-\\int_{t_1}^{t_0} a(t)^T \\dfrac{\\delta f(t, h(t); \\theta_t)}{\\delta \\theta}dt $$\n",
    "\n",
    "$$\\frac{dL}{dt_1} = a(t_1)f(a(t_1), t_1, \\theta)$$\n",
    "\n",
    "$$\\frac{dL}{dt_0} = a(t_1) -\\int_{t_1}^{t_0} a(t)^T \\dfrac{\\delta f(t, h(t); \\theta_t)}{\\delta t}dt $$\n",
    "\n",
    "To sum it beautifully:\n",
    "\n",
    "> _Backpropagation using the adjoint method: the adjoint state is a vector with its own time-dependent dynamics, only the trajectory runs backward in time. We can solve for the gradients using an ODE solver for the adjoint time derivative, starting at $t_1$._ [source](https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html)\n",
    "\n",
    "### Combining it all together!\n",
    "\n",
    "[source](https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html)\n",
    "\n",
    "How would we solve a problem using this approach?\n",
    "\n",
    "1. Instead of modelling this relationship direcatly, we model derivative:\n",
    "\n",
    "$$\\frac{dy}{dx} = f(x, y)$$\n",
    "\n",
    "2. We parametrise approximation by a neural network with hidden states $h(t)$, with $h(t_0) = x$, $h(t_1) = y$. Function approximation problem now has form of (f is a neural network):\n",
    "\n",
    "$$\\frac{dh(t)}{dt} = f(t, h(t), \\theta)$$\n",
    "\n",
    "\n",
    "3. Based on the above we can numerically integrate state, so we get network response:\n",
    "\n",
    "$$\\hat{y} = h(t_1) = \\int_{t_0}^{t_1} f(t, h(t), \\theta)dt$$.\n",
    "\n",
    "4. Free parameters of our problem are $t_0, t_1, \\theta$. We optimize free parameters with the help of the adjoint method.\n",
    "\n",
    "### Wait - but why?\n",
    "\n",
    "- Adaptive computation\n",
    "- Parameter efficency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "class NeuralODE:\n",
    "    def __init__(self, model: tf.keras.Model, t=[0,1]):\n",
    "        self._t = t\n",
    "        self._model = model\n",
    "        self.__output_dim_product = None\n",
    "        self.__output_gradients_dim_product = None\n",
    "        self.__batch_size = None\n",
    "        self.__number_of_parameters = None\n",
    "        \n",
    "    \n",
    "    def forward_pass(self, inputs, rtol=1e-06, atol=1e-12):\n",
    "        out = tf.contrib.integrate.odeint(\n",
    "            func = lambda _y, _t: self._model(inputs=(_t, _y)),\n",
    "            y0=inputs,\n",
    "            t=tf.to_float(self._t),\n",
    "            rtol=rtol,\n",
    "            atol=atol,\n",
    "            full_output=True\n",
    "        )\n",
    "        return out\n",
    "    \n",
    "    def backward_pass(self, outputs, output_gradients, rtol=1e-06, atol=1e-12):\n",
    "        # Ugly code, should be improved\n",
    "        self.__output_dim_product = self.__number_of_params(outputs)\n",
    "        self.__output_gradients_dim_product = self.__number_of_params(output_gradients)\n",
    "        self.__batch_size = self.__raw_shape(outputs)[0]\n",
    "        flat_state_tensor = self.__flat_state_tensor(output_gradients,outputs)\n",
    "        self.__number_of_parameters = self.__number_of_params(flat_state_tensor) - self.__output_gradients_dim_product - self.__output_dim_product\n",
    "        # TO IMPLEMENT PART\n",
    "        out, info_dict = tf.contrib.integrate.odeint(\n",
    "            func = lambda _y, _t: -1 * self._backward_dynamics(_t,  _y),\n",
    "            y0=flat_state_tensor,\n",
    "            t=tf.cast([0, 1], tf.float32),\n",
    "            rtol=rtol,\n",
    "            atol=atol,\n",
    "            full_output=True\n",
    "        )\n",
    "\n",
    "        layers_dims = [self.__raw_shape(weight) for weight in self._model.weights]\n",
    "        layers_size = [self.__number_of_params(weight) for weight in self._model.weights]\n",
    "        adjoint, hidden, gradients = tf.split(\n",
    "            out[-1], \n",
    "            [self.__output_gradients_dim_product , self.__output_dim_product, np.sum(layers_size)]\n",
    "        )\n",
    "        layer_params_dims = zip(tf.split(gradients, layers_size), layers_dims)\n",
    "        reshaped = [tf.reshape(param_dim[0], param_dim[1]) for param_dim in layer_params_dims]\n",
    "        return [\n",
    "            tf.reshape(hidden, [self.__batch_size, -1]), *reshaped\n",
    "        ]\n",
    "    # TO IMPLEMENT PART\n",
    "    def _backward_dynamics(self, t, flat_state_tensor):\n",
    "        adjoint, hidden, layers = tf.split(\n",
    "            flat_state_tensor, \n",
    "            [self.__output_gradients_dim_product, self.__output_dim_product, self.__number_of_parameters]\n",
    "        )\n",
    "        # WHAT ARE THOSE?\n",
    "        at = -tf.reshape(adjoint, [self.__batch_size, -1])\n",
    "        ht = tf.reshape(hidden, [self.__batch_size, -1])\n",
    "        with tf.GradientTape() as g:\n",
    "            g.watch(ht)\n",
    "            ht_new = self._model(inputs=(t, ht))\n",
    "        # WHY THIS GRADIENT?\n",
    "        gradients = g.gradient(\n",
    "            target=ht_new, \n",
    "            sources= [ht] + self._model.weights,\n",
    "            output_gradients=at\n",
    "        )\n",
    "        flat_layers = [tf.reshape(weight, [-1]) for weight in gradients]\n",
    "        resulting_state = tf.concat([adjoint, *flat_layers], axis=0)\n",
    "        return resulting_state\n",
    "    \n",
    "    def __flat_state_tensor(self, output_gradients, outputs):\n",
    "        grad_weights = [tf.zeros_like(w) for w in self._model.weights]\n",
    "        flat_layers = [tf.reshape(weight, [-1]) for weight in grad_weights]\n",
    "        state = (tf.reshape(output_gradients, [-1]), tf.reshape(outputs, [-1]), *flat_layers)\n",
    "        flat_state_tensor = tf.concat(state, 0)\n",
    "        return flat_state_tensor\n",
    "    \n",
    "        \n",
    "    def __number_of_params(self, tensor):\n",
    "        raw_dims = self.__raw_shape(tensor)\n",
    "        return np.prod(raw_dims)\n",
    "    \n",
    "    def __raw_shape(self, tensor):\n",
    "        s = tensor.get_shape()\n",
    "        raw_dims = tuple([s[i].value for i in range(0, len(s))])\n",
    "        return raw_dims\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Sample problem - MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "{'num_func_evals': <tf.Tensor: id=292552, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=292547, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.2232766 , 0.18914576, 0.37209553, 0.5840182 ,\n",
      "       0.8200452 , 1.0748899 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=292553, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0641342e+03, 1.3534608e+00, 6.9748753e-01, 2.8312925e-01,\n",
      "       3.4458068e-01, 4.0238574e-01, 4.3884486e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=298088, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=298083, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22328267, 0.18882613, 0.37140164, 0.5819026 ,\n",
      "       0.81689525, 1.0714382 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=298089, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0639896e+03, 1.3651400e+00, 6.9873136e-01, 2.8983745e-01,\n",
      "       3.4057432e-01, 3.9598444e-01, 4.3891084e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=303466, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=303461, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.2233091 , 0.18897207, 0.371732  , 0.58250964,\n",
      "       0.81818235, 1.0730407 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=303467, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0633605e+03, 1.3606812e+00, 6.9790369e-01, 2.8939763e-01,\n",
      "       3.3790016e-01, 3.9926845e-01, 4.3797046e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=308844, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=308839, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22326198, 0.18894263, 0.37122723, 0.58194554,\n",
      "       0.81670105, 1.0712702 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=308845, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0644825e+03, 1.3603063e+00, 7.0649993e-01, 2.8605634e-01,\n",
      "       3.4406799e-01, 3.9378873e-01, 4.4115540e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=314222, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=314217, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22328143, 0.18887292, 0.37100798, 0.5815496 ,\n",
      "       0.816197  , 1.0704095 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=314223, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0640189e+03, 1.3634120e+00, 7.0809740e-01, 2.8608203e-01,\n",
      "       3.4341836e-01, 3.9564535e-01, 4.3752134e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=319805, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=319800, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22331694, 0.18896635, 0.37102276, 0.5810419 ,\n",
      "       0.8151381 , 1.0699176 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=319806, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0631737e+03, 1.3611269e+00, 7.1138543e-01, 2.8903344e-01,\n",
      "       3.4319013e-01, 3.8668922e-01, 4.4333348e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=325183, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=325178, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.2233349 , 0.18894579, 0.3713276 , 0.58175373,\n",
      "       0.8175129 , 1.0723014 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=325184, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0627460e+03, 1.3624147e+00, 7.0467806e-01, 2.8881639e-01,\n",
      "       3.3447829e-01, 4.0055040e-01, 4.3915638e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=330561, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=330556, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22325219, 0.18879254, 0.3706903 , 0.58032763,\n",
      "       0.81494963, 1.0689807 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=330562, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0647161e+03, 1.3654231e+00, 7.1121335e-01, 2.9040611e-01,\n",
      "       3.3628857e-01, 3.9684629e-01, 4.3845794e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=335939, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=335934, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22330508, 0.18891987, 0.37079912, 0.58129144,\n",
      "       0.81603545, 1.0706435 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=335940, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0634558e+03, 1.3624396e+00, 7.1397823e-01, 2.8441128e-01,\n",
      "       3.4231126e-01, 3.9339048e-01, 4.3944374e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=341317, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=341312, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22324868, 0.18896468, 0.37172976, 0.58320963,\n",
      "       0.818624  , 1.0732191 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=341318, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0647998e+03, 1.3591074e+00, 6.9766927e-01, 2.8466433e-01,\n",
      "       3.4545606e-01, 3.9914188e-01, 4.4143414e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=346900, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=346895, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22325437, 0.18910515, 0.37176782, 0.58205366,\n",
      "       0.81647384, 1.0711614 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=346901, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0646638e+03, 1.3542397e+00, 7.0223182e-01, 2.9201871e-01,\n",
      "       3.4299523e-01, 3.9007577e-01, 4.4002765e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=352278, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=352273, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22328644, 0.18891111, 0.37109166, 0.58031595,\n",
      "       0.8154162 , 1.0696648 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=352279, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0638995e+03, 1.3621864e+00, 7.0792919e-01, 2.9557097e-01,\n",
      "       3.2961541e-01, 3.9919508e-01, 4.3696347e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=357656, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=357651, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22335747, 0.18893984, 0.37089816, 0.58139396,\n",
      "       0.8166601 , 1.0710897 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=357657, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0622091e+03, 1.3633183e+00, 7.1280479e-01, 2.8500676e-01,\n",
      "       3.3855721e-01, 3.9918238e-01, 4.3963513e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=362829, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=362824, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22326982, 0.18879665, 0.37094337, 0.581609  ,\n",
      "       0.81634176, 1.0705235 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=362830, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0642953e+03, 1.3658129e+00, 7.0644313e-01, 2.8533217e-01,\n",
      "       3.4380537e-01, 3.9660582e-01, 4.3766570e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=368207, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=368202, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22324029, 0.1883087 , 0.37037563, 0.58029675,\n",
      "       0.81355053, 1.0672153 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=368208, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0650002e+03, 1.3826859e+00, 6.9889051e-01, 2.8979239e-01,\n",
      "       3.4861803e-01, 3.8819936e-01, 4.3541533e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=373585, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=373580, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22325383, 0.18866651, 0.37120265, 0.58143353,\n",
      "       0.8164072 , 1.0706848 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=373586, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0646769e+03, 1.3700393e+00, 6.9653404e-01, 2.9138979e-01,\n",
      "       3.3853161e-01, 3.9789423e-01, 4.3641225e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=378963, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=378958, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22326115, 0.18855926, 0.3703006 , 0.57843703,\n",
      "       0.812109  , 1.0655216 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=378964, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0645024e+03, 1.3741649e+00, 7.0987761e-01, 2.9973689e-01,\n",
      "       3.3107021e-01, 3.9364469e-01, 4.3126729e-01], dtype=float32)>}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_func_evals': <tf.Tensor: id=384341, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=384336, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22332443, 0.18883575, 0.37134784, 0.58152217,\n",
      "       0.8162141 , 1.0707641 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=384342, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0629950e+03, 1.3660688e+00, 7.0012432e-01, 2.9159003e-01,\n",
      "       3.4011075e-01, 3.9340273e-01, 4.3982181e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=389719, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=389714, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22330394, 0.1889291 , 0.3713855 , 0.5798601 ,\n",
      "       0.8136233 , 1.0674163 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=389720, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0634833e+03, 1.3620726e+00, 7.0292854e-01, 3.0320898e-01,\n",
      "       3.3311722e-01, 3.9146703e-01, 4.3356884e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=395097, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=395092, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.2233214 , 0.18876638, 0.3710897 , 0.5801423 ,\n",
      "       0.8140446 , 1.0684233 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=395098, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0630671e+03, 1.3684881e+00, 7.0246476e-01, 2.9795137e-01,\n",
      "       3.3675855e-01, 3.8813350e-01, 4.3914324e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=400475, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=400470, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22331688, 0.1888212 , 0.3715154 , 0.5820075 ,\n",
      "       0.81735456, 1.0719938 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=400476, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0631746e+03, 1.3663647e+00, 6.9637442e-01, 2.9084155e-01,\n",
      "       3.3794677e-01, 3.9822540e-01, 4.3960145e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=406058, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=406053, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22330695, 0.18899098, 0.3710364 , 0.5811397 ,\n",
      "       0.81583136, 1.0704921 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=406059, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0634113e+03, 1.3599358e+00, 7.1206385e-01, 2.8836817e-01,\n",
      "       3.3953789e-01, 3.9254570e-01, 4.4037876e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=411436, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=411431, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.2233751 , 0.18895228, 0.371705  , 0.5825816 ,\n",
      "       0.81727964, 1.0713272 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=411437, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0617899e+03, 1.3634073e+00, 6.9767618e-01, 2.8866249e-01,\n",
      "       3.4578559e-01, 3.9736003e-01, 4.3189916e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=416814, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=416809, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22330587, 0.18877265, 0.37111622, 0.58116496,\n",
      "       0.81628025, 1.0707182 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=416815, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0634374e+03, 1.3677850e+00, 7.0219165e-01, 2.9111469e-01,\n",
      "       3.3605382e-01, 3.9783898e-01, 4.3983871e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=422192, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=422187, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22328037, 0.18880522, 0.37105888, 0.58115983,\n",
      "       0.8158629 , 1.0704808 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=422193, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0640441e+03, 1.3658249e+00, 7.0453280e-01, 2.9003748e-01,\n",
      "       3.3943665e-01, 3.9297199e-01, 4.3899170e-01], dtype=float32)>}\n",
      "{'num_func_evals': <tf.Tensor: id=427834, shape=(), dtype=int32, numpy=43>, 'integrate_points': <tf.Tensor: id=427829, shape=(7,), dtype=float32, numpy=\n",
      "array([1.        , 0.22334151, 0.18880811, 0.37033355, 0.5786635 ,\n",
      "       0.81226605, 1.0660336 ], dtype=float32)>, 'error_ratio': <tf.Tensor: id=427835, shape=(7,), dtype=float32, numpy=\n",
      "array([1.0625885e+03, 1.3675921e+00, 7.1883392e-01, 2.9657927e-01,\n",
      "       3.3310705e-01, 3.9031899e-01, 4.3396795e-01], dtype=float32)>}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8923f4ea19e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0min_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mnode_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneural_ode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mlast_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-eed5e2fed3e7>\u001b[0m in \u001b[0;36mforward_pass\u001b[1;34m(self, inputs, rtol, atol)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mrtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0matol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0matol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mfull_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         )\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\integrate\\python\\ops\\odes.py\u001b[0m in \u001b[0;36modeint\u001b[1;34m(func, y0, t, rtol, atol, method, options, full_output, name)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[0mfull_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m         **options)\n\u001b[0m\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\integrate\\python\\ops\\odes.py\u001b[0m in \u001b[0;36m_dopri5\u001b[1;34m(func, y0, t, rtol, atol, full_output, first_step, safety, ifactor, dfactor, max_num_steps, name)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m___\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mnum_times\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[0minterpolate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msolution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrk_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         name='interpolate_loop')\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   3461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3462\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3463\u001b[1;33m         \u001b[0mloop_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3464\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3465\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\integrate\\python\\ops\\odes.py\u001b[0m in \u001b[0;36minterpolate\u001b[1;34m(solution, history, rk_state, i)\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[1;32mlambda\u001b[0m \u001b[0mrk_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mrk_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[0madaptive_runge_kutta_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m             name='integrate_loop')\n\u001b[0m\u001b[0;32m    384\u001b[0m         y = _interp_evaluate(rk_state.interp_coeff, rk_state.t0, rk_state.t1,\n\u001b[0;32m    385\u001b[0m                              t[i])\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   3461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3462\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3463\u001b[1;33m         \u001b[0mloop_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3464\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3465\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\integrate\\python\\ops\\odes.py\u001b[0m in \u001b[0;36madaptive_runge_kutta_step\u001b[1;34m(rk_state, history, n_steps)\u001b[0m\n\u001b[0;32m    345\u001b[0m       with ops.control_dependencies(\n\u001b[0;32m    346\u001b[0m           [check_underflow, check_max_num_steps, check_numerics]):\n\u001b[1;32m--> 347\u001b[1;33m         \u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_runge_kutta_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error_ratio'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\integrate\\python\\ops\\odes.py\u001b[0m in \u001b[0;36m_runge_kutta_step\u001b[1;34m(func, y0, f0, t0, dt, tableau, name)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0malpha_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtableau\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableau\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m       \u001b[0mti\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malpha_i\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m       \u001b[0myi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0m_scaled_dot_product\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt_cast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m       \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mti\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mr_binary_op_wrapper\u001b[1;34m(y, x)\u001b[0m\n\u001b[0;32m    908\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m   \u001b[1;31m# Propagate func.__doc__ to the wrappers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1178\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Case: Dense * Sparse.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6860\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m   6861\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Mul\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6862\u001b[1;33m         name, _ctx._post_execution_callbacks, x, y)\n\u001b[0m\u001b[0;32m   6863\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6864\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.0001\n",
    "num_steps = 100\n",
    "batch_size = 128\n",
    "display_step = 300\n",
    "\n",
    "# Network Parameters\n",
    "node_dim = 256 # 1st layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "in_layer = tf.keras.layers.Dense(node_dim)\n",
    "out_layer = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "class Lambda(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Lambda, self).__init__(name=\"Module\")\n",
    "        self.dense_1 = tf.keras.layers.Dense(node_dim)\n",
    "        self.dense_2 = tf.keras.layers.Dense(node_dim)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        t, x = inputs\n",
    "        h = self.dense_1(x)\n",
    "        return  self.dense_2(h)\n",
    "\n",
    "neural_ode = NeuralODE(Lambda())\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "loss_history = []\n",
    "for step in range(num_steps):\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    in_result = in_layer(batch_x)\n",
    "    node_output, info_dict = neural_ode.forward_pass(in_result)\n",
    "    last_step = node_output[-1]\n",
    "    prediction = out_layer(last_step)\n",
    "    with tf.GradientTape() as g: \n",
    "        g.watch(prediction)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=batch_y))\n",
    "    loss_history.append(loss.numpy())\n",
    "    dLossdLast = g.gradient(loss, prediction)\n",
    "    dWeightsdOut = tf.matmul(tf.transpose(last_step), dLossdLast)\n",
    "    \n",
    "    dLoss = tf.matmul(dLossdLast, tf.transpose(out_layer.weights[0]))\n",
    "    hidden, *dWeightsdODE = neural_ode.backward_pass(last_step, dLoss) \n",
    "    \n",
    "    back_hidden = tf.matmul(hidden, tf.transpose(neural_ode._model.dense_2.weights[0]))\n",
    "    back_hidden = tf.matmul(back_hidden, neural_ode._model.dense_1.weights[0])\n",
    "    step_back = tf.matmul(back_hidden, tf.transpose(in_layer.weights[0]))\n",
    "    dWeightsdIn = tf.matmul(tf.transpose(step_back), in_result)\n",
    "\n",
    "    optimizer.apply_gradients(zip(dWeightsdODE, neural_ode._model.trainable_weights))\n",
    "    optimizer.apply_gradients(zip([dWeightsdOut, tf.reduce_mean(dLossdLast, axis=0)], out_layer.trainable_weights))\n",
    "    optimizer.apply_gradients(zip([dWeightsdIn, tf.reduce_mean(hidden, axis=0)], in_layer.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPH ERRORS\n",
    "print(loss_history)\n",
    "plt.plot(loss_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### More advanced problems\n",
    "\n",
    "- Scalable and intertivle normalizing flows\n",
    "- Continous time-sries models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Sources:\n",
    " - https://github.com/kmkolasinski/deep-learning-notes/tree/master/seminars/2019-03-Neural-Ordinary-Differential-Equations\n",
    " - https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/\n",
    " - https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html\n",
    " - https://arxiv.org/abs/1806.07366\n",
    " - https://pl.wikipedia.org/wiki/Regresja_(statystyka)\n",
    " - https://en.wikipedia.org/wiki/Ordinary_differential_equation\n",
    " - https://en.wikipedia.org/wiki/Initial_value_problem\n",
    " - http://tutorial.math.lamar.edu/Classes/DE/Modeling.aspx\n",
    " - https://en.wikipedia.org/wiki/Sensitivity_analysis\n",
    " - https://wizardforcel.gitbooks.io/tensorflow-101-sjchoi86/mlp_mnist_simple.html\n",
    " - Reprezentacja stanu sieci neuronowych przy pomocy zwyczajnych rwna rniczkowych - Piotr Lewandowski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
